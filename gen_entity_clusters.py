import pandas as pd
import rltk
import json

MAX_DISTANCE = 999999


class Cluster(object):
    def __init__(self, ds):
        self.attractive_records = set([])  # contribute to clustering
        self.all_records = set([])
        self.ds = ds
        self.type = None

    @staticmethod
    def record_score(r1, r2):
        score = rltk.jaccard_index_similarity(set(r1.concatenated_labels), set(r2.concatenated_labels))
        return score

    def distance(self, r):
        if r.type != self.type:
            return MAX_DISTANCE

        score = max([self.record_score(r, self.ds.get_record(rr)) for rr in self.attractive_records])
        if score == 0:
            return MAX_DISTANCE
        return 1 / score

    def add(self, r, contribute=True):
        if isinstance(r, rltk.Record):
            r = r.id
        if contribute:
            self.attractive_records.add(r)
        self.all_records.add(r)


def concat_list(*arr):
    ret = []
    for a in arr:
        if a:
            ret += a
    return ret


def concat_raw_object_list(ins, *keys):
    arr = []
    for k in keys:
        v = ins.raw_object[k]
        if isinstance(v, (list, tuple)):
            arr += v
        else:
            arr.append(v)
    return concat_list(arr)


@rltk.set_id('e')
class GaiaRecord(rltk.AutoGeneratedRecord):
    
    @rltk.cached_property
    def concatenated_labels(self):
        return concat_raw_object_list(
            self,
            'name', 'transl_name',
            'wiki_label_en', 'wiki_label_ru', 'wiki_label_uk', 
            'wiki_alias_en', 'wiki_alias_ru', 'wiki_alias_uk')


def gen_entity_clusters_baseline(entity_h5, outdir):
    df_entity = pd.read_hdf(entity_h5)
    df_entity['has_freebase'] = df_entity['target_type'].apply(lambda x: x == 'm')
    df_entity.head()

    set(df_entity['type'])

    df_entity = df_entity.loc[df_entity['type'].isin([
        'ldcOnt:Person',
        'ldcOnt:Person.MilitaryPersonnel',
        'ldcOnt:Person.MilitaryPersonnel.MilitaryOfficer',
        'ldcOnt:Organization',
        'ldcOnt:Organization.Association',
        'ldcOnt:Organization.Association.Club',
        'ldcOnt:Organization.CommercialOrganization.BroadcastingCompany',
        'ldcOnt:Organization.CommercialOrganization.Manufacturer',
        'ldcOnt:Organization.Government',
        'ldcOnt:Organization.Government.Agency',
        'ldcOnt:Organization.Government.LegislativeBody',
        'ldcOnt:Organization.International',
        'ldcOnt:Organization.MilitaryOrganization.GovernmentArmedForces',
        'ldcOnt:Organization.PoliticalOrganization.Party',
        'ldcOnt:GeopoliticalEntity',
        'ldcOnt:GeopoliticalEntity.Country.Country',
        'ldcOnt:GeopoliticalEntity.OrganizationOfCountries.OrganizationOfCountries',
        'ldcOnt:GeopoliticalEntity.UrbanArea.City',
        'ldcOnt:Location',
        'ldcOnt:Location.Land',
        'ldcOnt:Location.Land.Continent',
        'ldcOnt:Location.Position.Region',
        'ldcOnt:Commodity.Document'
    ])]
    df_entity[df_entity['wikidata'].notnull()].head()

    # # Create RLTK components
    ds = rltk.Dataset(reader=rltk.DataFrameReader(df_entity), record_class=GaiaRecord)

    bg = rltk.HashBlockGenerator()
    blocks = bg.block(ds, function_=lambda r: r.target if r.has_freebase else 'None')

    sum(1 for _ in blocks.key_set_adapter)

    num_in_block = []
    for b, data in blocks.key_set_adapter:
        num_in_block.append(len(data))

    from collections import Counter
    dict(sorted(Counter(num_in_block).items()))

    # Cluster baseline #

    # build cluster based on type
    all_clusters = []
    block_and_cluster = {}
    for bid, data in blocks.key_set_adapter:
        clusters = {}
        for _, r_id in data:
            r = ds.get_record(r_id)

            if bid == 'None':
                continue

            type_ = r.type
            if type_ not in clusters:
                c = Cluster(ds)
                c.type = type_
                c.add(r)
                clusters[type_] = c
            else:
                clusters[type_].add(r)

        block_and_cluster[bid] = clusters

        for c in clusters.values():
            all_clusters.append(c)

    bsize = set([])
    i = 0
    for k, v in block_and_cluster.items():
        bsize.add(len(v))
        print(k, v)
        i += 1
        if i == 100:
            break

    df_entity[df_entity['target'] == 'LDC2015E42:m.0372_h'][['e', 'type', 'name', 'source', 'target']].groupby('e').head(1)
    df_entity_xi = pd.read_hdf(outdir + '/entity_all.h5')
    df_entity_xi[df_entity_xi['e'] == 'http://www.isi.edu/gaia/entities/434638b2-8700-4f18-923d-d056815a5fb6']

    def debug_output(c, type_):
        j = {'attractive_records': list(c.attractive_records), 'all_records': {}, 'type': type_}
        for rid in c.all_records:
            j['all_records'][rid] = ds.get_record(rid).__dict__
        return j

    with open(outdir + '/entity-clusters.jl', 'w') as f:
        for c in all_clusters:
            f.write(json.dumps(list(c.all_records)) + '\n')
    with open(outdir + '/entity-clusters-debug.jl', 'w') as f:
        for c in all_clusters:
            f.write(json.dumps(debug_output(c, 'baseline')) + '\n')




